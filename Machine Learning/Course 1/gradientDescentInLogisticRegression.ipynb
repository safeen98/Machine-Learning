{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67a36f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71fb687",
   "metadata": {},
   "source": [
    "Defining our data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e8560a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x train (6, 2)\n",
      "Shape of y train (6,)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1])\n",
    "print(f\"Shape of x train {x_train.shape}\")\n",
    "print(f\"Shape of y train {y_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5929ff",
   "metadata": {},
   "source": [
    "Currently our data set has 6 training data with 2 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65f2a70",
   "metadata": {},
   "source": [
    "Logistic Gradient Descent\n",
    "\n",
    "Recall the gradient descent algorithm utilizes the gradient calculation:\n",
    "repeat until convergence:{ğ‘¤ğ‘—=ğ‘¤ğ‘—âˆ’ğ›¼âˆ‚ğ½(ğ°,ğ‘)âˆ‚ğ‘¤ğ‘—ğ‘=ğ‘âˆ’ğ›¼âˆ‚ğ½(ğ°,ğ‘)âˆ‚ğ‘}for j := 0..n-1(1)\n",
    "\n",
    "Where each iteration performs simultaneous updates on  ğ‘¤ğ‘—\n",
    "  for all  ğ‘—\n",
    " , where\n",
    "âˆ‚ğ½(ğ°,ğ‘)âˆ‚ğ‘¤ğ‘—âˆ‚ğ½(ğ°,ğ‘)âˆ‚ğ‘=1ğ‘šâˆ‘ğ‘–=0ğ‘šâˆ’1(ğ‘“ğ°,ğ‘(ğ±(ğ‘–))âˆ’ğ‘¦(ğ‘–))ğ‘¥(ğ‘–)ğ‘—=1ğ‘šâˆ‘ğ‘–=0ğ‘šâˆ’1(ğ‘“ğ°,ğ‘(ğ±(ğ‘–))âˆ’ğ‘¦(ğ‘–))(2)(3)\n",
    "\n",
    "m is the number of training examples in the data set\n",
    "ğ‘“ğ°,ğ‘(ğ‘¥(ğ‘–))\n",
    "  is the model's prediction, while  ğ‘¦(ğ‘–)\n",
    "  is the target\n",
    "For a logistic regression model\n",
    "ğ‘§=ğ°â‹…ğ±+ğ‘\n",
    " \n",
    "ğ‘“ğ°,ğ‘(ğ‘¥)=ğ‘”(ğ‘§)\n",
    " \n",
    "where  ğ‘”(ğ‘§)\n",
    "  is the sigmoid function:\n",
    "ğ‘”(ğ‘§)=11+ğ‘’âˆ’ğ‘§"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6c6b88",
   "metadata": {},
   "source": [
    "Gradient Descent Implementation\n",
    "The gradient descent algorithm implementation has two components:\n",
    "\n",
    "The loop implementing equation (1) above. This is gradient_descent below and is generally provided to you in optional and practice labs.\n",
    "The calculation of the current gradient, equations (2,3) above. This is compute_gradient_logistic below. You will be asked to implement this week's practice lab.\n",
    "Calculating the Gradient, Code Description\n",
    "Implements equation (2),(3) above for all  ğ‘¤ğ‘—\n",
    "  and  ğ‘\n",
    " . There are many ways to implement this. Outlined below is this:\n",
    "\n",
    "initialize variables to accumulate dj_dw and dj_db\n",
    "\n",
    "for each example\n",
    "\n",
    "calculate the error for that example  ğ‘”(ğ°â‹…ğ±(ğ‘–)+ğ‘)âˆ’ğ²(ğ‘–)\n",
    " \n",
    "for each input value  ğ‘¥(ğ‘–)ğ‘—\n",
    "  in this example,\n",
    "multiply the error by the input  ğ‘¥(ğ‘–)ğ‘—\n",
    " , and add to the corresponding element of dj_dw. (equation 2 above)\n",
    "add the error to dj_db (equation 3 above)\n",
    "divide dj_db and dj_dw by total number of examples (m)\n",
    "\n",
    "note that  ğ±(ğ‘–)\n",
    "  in numpy X[i,:] or X[i] and  ğ‘¥(ğ‘–)ğ‘—\n",
    "  is X[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0243446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x,y,w,b):\n",
    "    m,n=x.shape #determining the values of m (number of training data) and n (number of features)\n",
    "    dj_dw = np.zeros((n)) # determining the shape of partial derivative of w , this will be the size of the feature as it will be multipled to each feature\n",
    "    dj_db = 0\n",
    "    for i in range(m):\n",
    "        f_x = np.dot(x[i],w)+b\n",
    "        f_wb_i = 1/(1+np.exp(-f_x))\n",
    "        err = f_wb_i - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err*x[i,j]\n",
    "        dj_db = dj_db + err\n",
    "    dj_dw = dj_dw/m\n",
    "    dj_db = dj_db/m\n",
    "    return dj_db , dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ec2243d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db: 0.49861806546328574\n",
      "dj_dw: [0.498333393278696, 0.49883942983996693]\n"
     ]
    }
   ],
   "source": [
    "X_tmp = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y_tmp = np.array([0, 0, 0, 1, 1, 1])\n",
    "w_tmp = np.array([2.,3.])\n",
    "b_tmp = 1.\n",
    "dj_db_tmp, dj_dw_tmp = compute_gradient(X_tmp, y_tmp, w_tmp, b_tmp)\n",
    "print(f\"dj_db: {dj_db_tmp}\" )\n",
    "print(f\"dj_dw: {dj_dw_tmp.tolist()}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c9ae4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x,y,w_in,b_in,alpha,itr):\n",
    "    w=copy.deepcopy(w_in)\n",
    "    b=b_in\n",
    "    for i in range(itr):\n",
    "        dj_db,dj_dw= compute_gradient(x,y,w,b)\n",
    "        w = w - alpha*dj_dw\n",
    "        b = b - alpha*dj_db\n",
    "    return w,b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6cda971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of w and b [0.59887726 0.34495287] and -0.9519854836547768\n"
     ]
    }
   ],
   "source": [
    "w_in1 = np.zeros_like(x_train[0])\n",
    "b_in1 = 0\n",
    "alpha = 0.0001\n",
    "itrs = 100000\n",
    "b_out ,w_out = gradient_descent(x_train,y_train,w_in1,b_in1,alpha,itrs)\n",
    "print(f\"value of w and b {b_out} and {w_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da6bb075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x,w,b):\n",
    "    f_x = np.dot(x,w) +b\n",
    "    f_x_i = 1/(1+np.exp(-f_x))\n",
    "    return f_x_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "411da5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5 1.5] [0.59887726 0.34495287] -0.9519854836547768\n",
      "0.7182251555207515\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0],b_out,w_out)\n",
    "predicted = predict(x_train[4],b_out,w_out)\n",
    "print(predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
